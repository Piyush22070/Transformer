{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qgYaLgJFq02",
        "outputId": "9232974f-d347-4c88-f8ac-b1aefc0ccc9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ams71epeFGWG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Importing Libraries\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torcheval.metrics.functional import bleu_score\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7p1c6iOFpJk",
        "outputId": "427ca275-345c-4362-a7b5-c9baf52bf424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Model Hyper-parameters\n",
        "'''\n",
        "\n",
        "n_emb = 256\n",
        "vocab_size = 30000\n",
        "seq_len = 64\n",
        "batch_size = 64\n",
        "num_heads = 4\n",
        "n_dropout = 0.1\n",
        "ffwd_w = 1024\n",
        "num_sa_blocks = 4\n",
        "num_ca_blocks = 4\n",
        "\n",
        "\n",
        "torch.manual_seed()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RRfnDOn0F1E1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Tokenization\n",
        "'''\n",
        "\n",
        "# tokenizer = Tokenizer(models.BPE())\n",
        "# tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "\n",
        "# trainer = trainers.BpeTrainer(\n",
        "#         vocab_size = vocab_size,\n",
        "#         special_tokens = [\"[START]\", \"[END]\", \"[PAD]\"]\n",
        "#         )\n",
        "\n",
        "# tokenizer.train(files = [\"sample_data/train.txt\", \"sample_data/validation.txt\"], trainer=trainer)\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=vocab_size,\n",
        "    special_tokens=[\"[START]\", \"[END]\", \"[PAD]\"]\n",
        ")\n",
        "\n",
        "tokenizer.train(files=['sample_data/source_train.txt', 'sample_data/target_train.txt'], trainer=trainer)\n",
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ycAk83JHBG",
        "outputId": "7764a2a4-de99-4b1d-f6ca-83f30bde1a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "222312 222312 55579 55579\n",
            "Tom's bedtime is nine o'clock on school nights.\n",
            "Wenn Schule ist, beginnt Toms Schlafenszeit um neun Uhr.\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Opening Files and Embedding Vector\n",
        "'''\n",
        "\n",
        "# def read_data(file_path):\n",
        "#     inc, corr = [], []\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split('\\t')\n",
        "#             if len(parts) == 2:\n",
        "#                 inc.append(parts[0])\n",
        "#                 corr.append(parts[1])\n",
        "#     return inc, corr\n",
        "\n",
        "# trn_inc, trn_corr = read_data('sample_data/train.txt')\n",
        "\n",
        "# val_inc, val_corr = read_data('sample_data/validation.txt')\n",
        "\n",
        "# # same as of now\n",
        "# tst_inc, tst_corr = val_inc, val_corr\n",
        "\n",
        "# embeds = nn.Embedding(vocab_size, n_emb).to(device)\n",
        "\n",
        "# # Print lengths to check\n",
        "# print(len(trn_inc), len(trn_corr), len(tst_inc), len(tst_corr), len(val_inc), len(val_corr))\n",
        "# print(embeds)\n",
        "# print(trn_corr[0])\n",
        "\n",
        "'''\n",
        "Opening Files and Embedding Vector\n",
        "'''\n",
        "def load_lines(file_path, limit=None):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "    if limit:\n",
        "        lines = lines[:limit]\n",
        "    return lines\n",
        "\n",
        "trn_src = load_lines('sample_data/source_train.txt', limit=500000)\n",
        "trn_tgt = load_lines('sample_data/target_train.txt', limit=500000)\n",
        "\n",
        "val_src = load_lines('sample_data/source_valid.txt')\n",
        "val_tgt = load_lines('sample_data/target_valid.txt')\n",
        "\n",
        "# If you have test files, load similarly:\n",
        "# tst_src = load_lines('sample_data/source_test.txt')\n",
        "# tst_tgt = load_lines('sample_data/target_test.txt')\n",
        "\n",
        "# Step 4: Prepare embedding layer\n",
        "embeds = nn.Embedding(vocab_size, n_emb, device=device)\n",
        "\n",
        "# Print lengths to verify\n",
        "print(len(trn_src), len(trn_tgt), len(val_src), len(val_tgt))\n",
        "print(trn_src[0])\n",
        "print(trn_tgt[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cu9TCZebOARd"
      },
      "outputs": [],
      "source": [
        "def get_tensor(token_id, tokenizer, embeds, pos_encoder, device):\n",
        "    token_id = torch.tensor(token_id, device=device)\n",
        "    emb_token = embeds(token_id).to(device)\n",
        "    emb_token = emb_token.view(1, 1, -1)\n",
        "    emb_token = pos_encoder(emb_token, seq_len=1)\n",
        "    return emb_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2Yo-KWq3OeEe"
      },
      "outputs": [],
      "source": [
        "def get_tensor_for_inf_x(x, tokenizer, embeds, pos_encoder, seq_len, n_emb, device):\n",
        "    tokens = tokenizer.encode(x).tokens\n",
        "    tokens.insert(0, \"[START]\")\n",
        "    tokens.append(\"[END]\")\n",
        "\n",
        "    if len(tokens) > seq_len:\n",
        "        raise ValueError(f\"Input is too long: {len(tokens)} tokens, max is {seq_len}\")\n",
        "\n",
        "    while len(tokens) < seq_len:\n",
        "        tokens.append(\"[PAD]\")\n",
        "\n",
        "    token_ids = [tokenizer.token_to_id(t) for t in tokens]\n",
        "    t_tokens = torch.tensor(token_ids, dtype=torch.long, device=device)\n",
        "\n",
        "    t_embs = embeds(t_tokens).unsqueeze(0)  # (1, seq_len, n_emb)\n",
        "    t_embs = pos_encoder(t_embs)\n",
        "\n",
        "    return t_embs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RT2jbe4IOs46"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_batch(transformer, split=\"train\", batch_size=16, seq_len=64):\n",
        "    # Select dataset\n",
        "    inp_data = trn_src if split == \"train\" else val_src\n",
        "    out_data = trn_tgt if split == \"train\" else val_tgt\n",
        "\n",
        "    # Initialize tensors\n",
        "    inp_btc = torch.zeros(batch_size, seq_len, transformer.n_emb, device=transformer.device)\n",
        "    out_btc = torch.zeros(batch_size, seq_len, transformer.n_emb, device=transformer.device)\n",
        "    target = torch.empty(batch_size, seq_len, dtype=torch.long, device=transformer.device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Get random sentence pair that fits sequence length\n",
        "        while True:\n",
        "            idx = random.randint(0, len(inp_data) - 1)\n",
        "            inp = inp_data[idx]\n",
        "            out = out_data[idx]\n",
        "\n",
        "            inp_tokens = transformer.tokenizer.encode(inp).tokens\n",
        "            out_tokens = transformer.tokenizer.encode(out).tokens\n",
        "\n",
        "            inp_tokens = [\"[START]\"] + inp_tokens + [\"[END]\"]\n",
        "            out_tokens = [\"[START]\"] + out_tokens + [\"[END]\"]\n",
        "\n",
        "            if len(inp_tokens) <= seq_len and len(out_tokens) <= seq_len:\n",
        "                break\n",
        "\n",
        "        # Pad sequences to seq_len\n",
        "        inp_tokens += [\"[PAD]\"] * (seq_len - len(inp_tokens))\n",
        "        out_tokens += [\"[PAD]\"] * (seq_len - len(out_tokens))\n",
        "\n",
        "        # Prepare decoder input and target\n",
        "        dec_inp = out_tokens[:-1]\n",
        "        target_tokens = out_tokens[1:]\n",
        "\n",
        "        dec_inp += [\"[PAD]\"] * (seq_len - len(dec_inp))\n",
        "        target_tokens += [\"[PAD]\"] * (seq_len - len(target_tokens))\n",
        "\n",
        "        # Convert tokens to IDs\n",
        "        inp_ids = [transformer.tokenizer.token_to_id(tok) for tok in inp_tokens]\n",
        "        dec_ids = [transformer.tokenizer.token_to_id(tok) for tok in dec_inp]\n",
        "        tgt_ids = [transformer.tokenizer.token_to_id(tok) for tok in target_tokens]\n",
        "\n",
        "        # Convert to tensors\n",
        "        t_inp = torch.tensor(inp_ids, dtype=torch.long, device=transformer.device)\n",
        "        t_dec = torch.tensor(dec_ids, dtype=torch.long, device=transformer.device)\n",
        "        t_tgt = torch.tensor(tgt_ids, dtype=torch.long, device=transformer.device)\n",
        "\n",
        "        # Store target labels\n",
        "        target[i] = t_tgt\n",
        "\n",
        "        # Embed and apply positional encoding\n",
        "        inp_emb = transformer.pos_encoder(transformer.embeds(t_inp).unsqueeze(0)).squeeze(0)\n",
        "        dec_emb = transformer.pos_encoder(transformer.embeds(t_dec).unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # Store embeddings in batch\n",
        "        inp_btc[i] = inp_emb\n",
        "        out_btc[i] = dec_emb\n",
        "\n",
        "    return inp_btc, out_btc, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsCoPU60RGWs"
      },
      "outputs": [],
      "source": [
        "\n",
        "count = 0\n",
        "for sen in trn_src:\n",
        "    inp_tokens = tokenizer.encode(sen).tokens\n",
        "    inp_tokens.insert(0, \"[START]\")\n",
        "    inp_tokens.append(\"[END]\")\n",
        "\n",
        "    if len(inp_tokens) <= 64:\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6dvp9uORV3T",
        "outputId": "8fa993ec-59ce-4ff9-9301-dbb9733a1ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "222308\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sen in trn_tgt:\n",
        "    inp_tokens = tokenizer.encode(sen).tokens\n",
        "    inp_tokens.insert(0, \"[START]\")\n",
        "    inp_tokens.append(\"[END]\")\n",
        "\n",
        "    if len(inp_tokens) <= 64:\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7l_dcP4DRhLT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Positional Embedding\n",
        "'''\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=512, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.device = device\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model, device=device)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.size(1)\n",
        "        return x + self.pe[:seq_len, :].unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z1_TYdXgRyNW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Encoder\n",
        "'''\n",
        "\n",
        "class SA_Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.wk = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.wq = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.wv = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "\n",
        "    def forward(self, ini_emb):\n",
        "        k = self.wk(ini_emb)\n",
        "        q = self.wq(ini_emb)\n",
        "        v = self.wv(ini_emb)\n",
        "\n",
        "        k_mul = q @ k.transpose(-2, -1)\n",
        "        scaling = k_mul * (n_emb**-0.5)\n",
        "        sm_mul = F.softmax(scaling, dim=-1)\n",
        "        v_mul = sm_mul @ v\n",
        "\n",
        "        return v_mul\n",
        "\n",
        "\n",
        "class SA_MultiHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = n_emb // num_heads\n",
        "        self.heads = nn.ModuleList([SA_Head(self.head_size) for _ in range(num_heads)])\n",
        "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.lyr(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SA_Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mh_atn = SA_MultiHead()\n",
        "        self.dropout = nn.Dropout(n_dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_emb, device=device)\n",
        "        self.ln2 = nn.LayerNorm(n_emb, device=device)\n",
        "        self.ffwd = nn.Sequential(\n",
        "                nn.Linear(n_emb, ffwd_w, device=device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(ffwd_w, n_emb, device=device)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln1(x)\n",
        "        out = self.mh_atn(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out + x\n",
        "        out1 = self.ln2(out)\n",
        "        out1 = self.ffwd(out1)\n",
        "        out1 = out1 + out\n",
        "        return out1\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sa_blocks_list = []\n",
        "\n",
        "        for _ in range(num_sa_blocks):\n",
        "            self.sa_blocks_list.append(SA_Block())\n",
        "\n",
        "        self.sa_blocks = nn.Sequential(*self.sa_blocks_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.sa_blocks(x)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rWZZRb7STREl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Decoder\n",
        "'''\n",
        "\n",
        "class MA_Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.qw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.vw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.kw(x)\n",
        "        q = self.qw(x)\n",
        "        v = self.vw(x)\n",
        "        mask = torch.triu(torch.full((x.size(1), x.size(1)), float('-inf'), device=device), diagonal=1)\n",
        "        out = q @ k.transpose(-2, -1)\n",
        "        out = out * (n_emb  ** -0.5)\n",
        "        out = out + mask\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        out = out @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MA_MultiHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = n_emb // num_heads\n",
        "        self.heads = nn.ModuleList([MA_Head(self.head_size) for _ in range(num_heads)])\n",
        "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.lyr(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CA_Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.kw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.qw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "        self.vw = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
        "\n",
        "    def forward(self, x_enc, x_dec):\n",
        "        q = self.qw(x_dec)\n",
        "        k = self.kw(x_enc)\n",
        "        v = self.vw(x_enc)\n",
        "\n",
        "        out = q @ k.transpose(-2, -1)\n",
        "        out = out * (n_emb ** -0.5)\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        out = out @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CA_MultiHead(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = n_emb // num_heads\n",
        "        self.heads = nn.ModuleList([CA_Head(self.head_size) for _ in range(num_heads)])\n",
        "        self.lyr = nn.Linear(n_emb, n_emb, bias=False, device=device)\n",
        "\n",
        "\n",
        "    def forward(self, x_enc, x_dec):\n",
        "        out = torch.cat([h(x_enc, x_dec) for h in self.heads], dim=-1)\n",
        "        out = self.lyr(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CA_Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mmh_attn = MA_MultiHead()\n",
        "        self.do_1 = nn.Dropout(n_dropout)\n",
        "        self.cah_attn = CA_MultiHead()\n",
        "        self.do_2 = nn.Dropout(n_dropout)\n",
        "        self.ffwd = nn.Sequential(\n",
        "                nn.Linear(n_emb, ffwd_w, device=device),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(ffwd_w, n_emb, device=device)\n",
        "                )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(n_emb, device=device)\n",
        "        self.ln2 = nn.LayerNorm(n_emb, device=device)\n",
        "        self.ln3 = nn.LayerNorm(n_emb, device=device)\n",
        "\n",
        "    def forward(self, x_enc, x_dec):\n",
        "        out = self.ln1(x_dec)\n",
        "        out = self.mmh_attn(out)\n",
        "        out = self.do_1(out)\n",
        "        out = out + x_dec  # residual\n",
        "\n",
        "        out1 = self.ln2(out)\n",
        "        out1 = self.cah_attn(x_enc, out1)\n",
        "        out1 = self.do_2(out1)\n",
        "        out1 = out1 + out  # residual\n",
        "\n",
        "        out2 = self.ln3(out1)\n",
        "        out2 = self.ffwd(out2)\n",
        "        out2 = out2 + out1  # residual\n",
        "\n",
        "        return out2\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ca_blocks_list = []\n",
        "\n",
        "        for _ in range(num_ca_blocks):\n",
        "            self.ca_blocks_list.append(CA_Block())\n",
        "\n",
        "        self.ca_blocks = nn.ModuleList(self.ca_blocks_list)\n",
        "\n",
        "        self.lyr = nn.Linear(n_emb, vocab_size, device=device)\n",
        "\n",
        "    def forward(self, x_enc, x_dec, split=\"training\"):\n",
        "        # if purp == \"training\":\n",
        "        for block in self.ca_blocks:\n",
        "            x_dec = block(x_enc, x_dec)\n",
        "\n",
        "        '''\n",
        "        Final Layer\n",
        "        '''\n",
        "        if split == \"training\":\n",
        "          logits = self.lyr(x_dec)\n",
        "          out = F.softmax(logits, dim=-1)\n",
        "          out = torch.argmax(out, dim=-1)\n",
        "\n",
        "        if split == \"inference\":\n",
        "          logits = self.lyr(x_dec[:, -1, :])\n",
        "          out = F.softmax(logits, dim=-1)\n",
        "          out = torch.argmax(out, -1)\n",
        "\n",
        "        return logits, out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jektNJOaUURZ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        " Transformer Class\n",
        "'''\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, n_emb, seq_len, device, tokenizer):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(n_emb, seq_len, device)\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.embeds = nn.Embedding(vocab_size, n_emb, device=device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.n_emb = n_emb\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x_enc, x_dec, targets=None):\n",
        "        res_enc = self.encoder(x_enc)\n",
        "        logits, out = self.decoder(res_enc, x_dec)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets, ignore_index=self.tokenizer.token_to_id(\"[PAD]\"))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def get_batch(self, split=\"train\", batch_size=16):\n",
        "        return get_batch(self, split, batch_size, self.seq_len)\n",
        "\n",
        "    def predict(self, x, max_len=50):\n",
        "        x_dec = get_tensor(\n",
        "            self.tokenizer.token_to_id(\"[START]\"),\n",
        "            self.tokenizer,\n",
        "            self.embeds,\n",
        "            self.pos_encoder,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        inp_embs = get_tensor_for_inf_x(\n",
        "            x,\n",
        "            self.tokenizer,\n",
        "            self.embeds,\n",
        "            self.pos_encoder,\n",
        "            self.seq_len,\n",
        "            self.n_emb,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        res_enc = self.encoder(inp_embs)\n",
        "        final_sentence = []\n",
        "\n",
        "        while len(final_sentence) < max_len:\n",
        "            logits, out = self.decoder(res_enc, x_dec, split=\"inference\")\n",
        "            final_token = self.tokenizer.id_to_token(out[-1])\n",
        "\n",
        "            if final_token == \"[END]\":\n",
        "                break\n",
        "\n",
        "            final_sentence.append(final_token)\n",
        "\n",
        "            new_embed = get_tensor(\n",
        "                out[-1],\n",
        "                self.tokenizer,\n",
        "                self.embeds,\n",
        "                self.pos_encoder,\n",
        "                self.device\n",
        "            )\n",
        "\n",
        "            x_dec = torch.cat([x_dec, new_embed], dim=1)\n",
        "\n",
        "        return \" \".join(final_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JpuLhmhEVYOq"
      },
      "outputs": [],
      "source": [
        "def train_step(transformer, optimizer):\n",
        "    # Get batch\n",
        "    inp_btc, out_btc, targets = transformer.get_batch(\"train\")\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = transformer(inp_btc, out_btc, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JatMIM1rVsjB"
      },
      "outputs": [],
      "source": [
        "# Training loop example\n",
        "def train_transformer(transformer, lr, epochs=10):\n",
        "    optimizer = torch.optim.AdamW(transformer.parameters(), lr=lr, betas=(0.9, 0.98), weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=1, eta_min=1e-6)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        transformer.train()\n",
        "        train_losses = []\n",
        "\n",
        "        num_steps = len(trn_src) // batch_size\n",
        "        eval_interval = num_steps // 10\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            # loss = train_step(transformer, optimizer)\n",
        "            inp_btc, out_btc, targets = transformer.get_batch(\"train\")\n",
        "\n",
        "            # Forward pass\n",
        "            logits, loss = transformer(inp_btc, out_btc, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            if step % eval_interval == 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                print(f\"Epoch: {epoch+1} \\t | step: {step} \\t | lr: {current_lr} \\t | loss: {loss:.4f}\")\n",
        "\n",
        "        transformer.eval()\n",
        "        test_losses = []\n",
        "        for _ in range(100):\n",
        "            with torch.no_grad():\n",
        "                inp_btc, out_btc, targets = transformer.get_batch(\"test\")\n",
        "                _, eval_loss = transformer(inp_btc, out_btc, targets)\n",
        "                test_losses.append(eval_loss.item())\n",
        "\n",
        "        avg_trn_loss = sum(train_losses) / len(train_losses)\n",
        "        avg_tst_loss = sum(test_losses) / len(test_losses)\n",
        "\n",
        "        print(\"--------------------------------------\")\n",
        "        print(f\"Epoch {epoch+1} summary\")\n",
        "        print(f\"Avg Train loss: {avg_trn_loss}\")\n",
        "        print(f\"Avg Test loss: {avg_tst_loss}\")\n",
        "        print(\"--------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OsbB8NMcV5aQ"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    n_emb=n_emb,\n",
        "    seq_len=seq_len,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9q5Qk_PV-Gg",
        "outputId": "3bf7f029-1cd6-4736-bc41-273a7a158577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Total Parameters = 22750512\n",
            "\n",
            "Epoch: 1 \t | step: 0 \t | lr: 0.0002999992622476778 \t | loss: 10.6716\n",
            "Epoch: 1 \t | step: 347 \t | lr: 0.00021920718916291243 \t | loss: 6.6774\n",
            "Epoch: 1 \t | step: 694 \t | lr: 6.453671481952984e-05 \t | loss: 5.6903\n",
            "Epoch: 1 \t | step: 1041 \t | lr: 0.0002987004908448912 \t | loss: 4.9665\n",
            "Epoch: 1 \t | step: 1388 \t | lr: 0.0002015829701297767 \t | loss: 4.8856\n",
            "Epoch: 1 \t | step: 1735 \t | lr: 4.953774518590683e-05 \t | loss: 4.4700\n",
            "Epoch: 1 \t | step: 2082 \t | lr: 0.0002949463515266163 \t | loss: 5.5061\n",
            "Epoch: 1 \t | step: 2429 \t | lr: 0.0001831124145887831 \t | loss: 4.5493\n",
            "Epoch: 1 \t | step: 2776 \t | lr: 3.621150605224784e-05 \t | loss: 4.4871\n",
            "Epoch: 1 \t | step: 3123 \t | lr: 0.00028879904242175145 \t | loss: 5.2224\n",
            "Epoch: 1 \t | step: 3470 \t | lr: 0.00016410154048358366 \t | loss: 4.0392\n",
            "--------------------------------------\n",
            "Epoch 1 summary\n",
            "Avg Train loss: 5.0827928327395115\n",
            "Avg Test loss: 4.516445455551147\n",
            "--------------------------------------\n",
            "Epoch: 2 \t | step: 0 \t | lr: 0.00016269779642943955 \t | loss: 4.4799\n",
            "Epoch: 2 \t | step: 347 \t | lr: 2.4021937702657603e-05 \t | loss: 5.0530\n",
            "Epoch: 2 \t | step: 694 \t | lr: 0.0002796565508703288 \t | loss: 4.2335\n",
            "Epoch: 2 \t | step: 1041 \t | lr: 0.00014345758561890837 \t | loss: 4.5335\n",
            "Epoch: 2 \t | step: 1388 \t | lr: 1.4831078531103008e-05 \t | loss: 4.3191\n",
            "Epoch: 2 \t | step: 1735 \t | lr: 0.00026891545383037004 \t | loss: 4.3376\n",
            "Epoch: 2 \t | step: 2082 \t | lr: 0.00012433405268319622 \t | loss: 3.6579\n",
            "Epoch: 2 \t | step: 2429 \t | lr: 7.887965740174678e-06 \t | loss: 4.0988\n",
            "Epoch: 2 \t | step: 2776 \t | lr: 0.00025621246378738883 \t | loss: 4.2660\n",
            "Epoch: 2 \t | step: 3123 \t | lr: 0.0001056440340169292 \t | loss: 3.3748\n",
            "Epoch: 2 \t | step: 3470 \t | lr: 3.307631987883802e-06 \t | loss: 3.8891\n",
            "--------------------------------------\n",
            "Epoch 2 summary\n",
            "Avg Train loss: 4.177273327632215\n",
            "Avg Test loss: 4.016320910453796\n",
            "--------------------------------------\n",
            "Epoch: 3 \t | step: 0 \t | lr: 3.067564633766165e-06 \t | loss: 3.9618\n",
            "Epoch: 3 \t | step: 347 \t | lr: 0.00024063796648250916 \t | loss: 4.5083\n",
            "Epoch: 3 \t | step: 694 \t | lr: 8.642134232561831e-05 \t | loss: 3.9033\n",
            "Epoch: 3 \t | step: 1041 \t | lr: 1.1062238402319138e-06 \t | loss: 3.6329\n",
            "Epoch: 3 \t | step: 1388 \t | lr: 0.0002245710709306449 \t | loss: 4.9886\n",
            "Epoch: 3 \t | step: 1735 \t | lr: 6.960237280703206e-05 \t | loss: 4.0510\n",
            "Epoch: 3 \t | step: 2082 \t | lr: 0.00029937997883017523 \t | loss: 4.1693\n",
            "Epoch: 3 \t | step: 2429 \t | lr: 0.00020727697478050924 \t | loss: 5.0727\n",
            "Epoch: 3 \t | step: 2776 \t | lr: 5.4123705450175106e-05 \t | loss: 3.3340\n",
            "Epoch: 3 \t | step: 3123 \t | lr: 0.0002963995559098427 \t | loss: 3.4587\n",
            "Epoch: 3 \t | step: 3470 \t | lr: 0.00018904220453800193 \t | loss: 3.7545\n",
            "--------------------------------------\n",
            "Epoch 3 summary\n",
            "Avg Train loss: 3.8738520894378743\n",
            "Avg Test loss: 3.843210048675537\n",
            "--------------------------------------\n",
            "Epoch: 4 \t | step: 0 \t | lr: 0.00018767913813114574 \t | loss: 3.8000\n",
            "Epoch: 4 \t | step: 347 \t | lr: 3.929515309384974e-05 \t | loss: 3.7710\n",
            "Epoch: 4 \t | step: 694 \t | lr: 0.00029051420806885475 \t | loss: 2.6102\n",
            "Epoch: 4 \t | step: 1041 \t | lr: 0.00016877126209101343 \t | loss: 3.3788\n",
            "Epoch: 4 \t | step: 1388 \t | lr: 2.738187665455009e-05 \t | loss: 3.3188\n",
            "Epoch: 4 \t | step: 1735 \t | lr: 0.00028262296169825966 \t | loss: 3.4861\n",
            "Epoch: 4 \t | step: 2082 \t | lr: 0.0001495606699771489 \t | loss: 3.3335\n",
            "Epoch: 4 \t | step: 2429 \t | lr: 1.7508406481438497e-05 \t | loss: 4.5172\n",
            "Epoch: 4 \t | step: 2776 \t | lr: 0.000272542717982219 \t | loss: 2.5398\n",
            "Epoch: 4 \t | step: 3123 \t | lr: 0.00013036564057017567 \t | loss: 4.1944\n",
            "Epoch: 4 \t | step: 3470 \t | lr: 9.838325041343311e-06 \t | loss: 3.2593\n",
            "--------------------------------------\n",
            "Epoch 4 summary\n",
            "Avg Train loss: 3.6697304257047714\n",
            "Avg Test loss: 3.529547145366669\n",
            "--------------------------------------\n",
            "Epoch: 5 \t | step: 0 \t | lr: 9.367296110913076e-06 \t | loss: 3.1266\n",
            "Epoch: 5 \t | step: 347 \t | lr: 0.00025948080979950103 \t | loss: 3.9023\n",
            "Epoch: 5 \t | step: 694 \t | lr: 0.00011014571997815577 \t | loss: 3.2850\n",
            "Epoch: 5 \t | step: 1041 \t | lr: 4.202154803341925e-06 \t | loss: 2.6764\n",
            "Epoch: 5 \t | step: 1388 \t | lr: 0.00024543253130959974 \t | loss: 3.4771\n",
            "Epoch: 5 \t | step: 1735 \t | lr: 9.198963680783825e-05 \t | loss: 4.3082\n",
            "Epoch: 5 \t | step: 2082 \t | lr: 1.4608586068973694e-06 \t | loss: 3.3929\n",
            "Epoch: 5 \t | step: 2429 \t | lr: 0.0002298114220538567 \t | loss: 3.1408\n",
            "Epoch: 5 \t | step: 2776 \t | lr: 7.48029463007067e-05 \t | loss: 3.0189\n",
            "Epoch: 5 \t | step: 3123 \t | lr: 0.0002998111750125996 \t | loss: 3.7697\n",
            "Epoch: 5 \t | step: 3470 \t | lr: 0.00021287629069243388 \t | loss: 3.7421\n",
            "--------------------------------------\n",
            "Epoch 5 summary\n",
            "Avg Train loss: 3.5180407046859834\n",
            "Avg Test loss: 3.6872526931762697\n",
            "--------------------------------------\n",
            "Epoch: 6 \t | step: 0 \t | lr: 0.00021159303667308416 \t | loss: 2.9454\n",
            "Epoch: 6 \t | step: 347 \t | lr: 5.776114968250487e-05 \t | loss: 3.9662\n",
            "Epoch: 6 \t | step: 694 \t | lr: 0.00029735194398393893 \t | loss: 3.4022\n",
            "Epoch: 6 \t | step: 1041 \t | lr: 0.00019356035583420795 \t | loss: 3.9353\n",
            "Epoch: 6 \t | step: 1388 \t | lr: 4.3467495788236226e-05 \t | loss: 2.9466\n",
            "Epoch: 6 \t | step: 1735 \t | lr: 0.00029253711235556533 \t | loss: 3.5420\n",
            "Epoch: 6 \t | step: 2082 \t | lr: 0.00017481425619663508 \t | loss: 3.9494\n",
            "Epoch: 6 \t | step: 2429 \t | lr: 3.094714355618e-05 \t | loss: 2.5279\n",
            "Epoch: 6 \t | step: 2776 \t | lr: 0.00028536902692554626 \t | loss: 2.5202\n",
            "Epoch: 6 \t | step: 3123 \t | lr: 0.0001556653208854836 \t | loss: 3.8347\n",
            "Epoch: 6 \t | step: 3470 \t | lr: 2.0407528676905906e-05 \t | loss: 2.7646\n",
            "--------------------------------------\n",
            "Epoch 6 summary\n",
            "Avg Train loss: 3.405478712189551\n",
            "Avg Test loss: 3.3338788843154905\n",
            "--------------------------------------\n",
            "Epoch: 7 \t | step: 0 \t | lr: 1.971906179214882e-05 \t | loss: 2.9850\n",
            "Epoch: 7 \t | step: 347 \t | lr: 0.0002751947258251691 \t | loss: 3.5723\n",
            "Epoch: 7 \t | step: 694 \t | lr: 0.00013502870168506182 \t | loss: 2.4946\n",
            "Epoch: 7 \t | step: 1041 \t | lr: 1.1498415359706406e-05 \t | loss: 3.4433\n",
            "Epoch: 7 \t | step: 1388 \t | lr: 0.00026356840184759507 \t | loss: 3.5791\n",
            "Epoch: 7 \t | step: 1735 \t | lr: 0.0001160567807118497 \t | loss: 3.4159\n",
            "Epoch: 7 \t | step: 2082 \t | lr: 5.580730470657426e-06 \t | loss: 2.9321\n",
            "Epoch: 7 \t | step: 2429 \t | lr: 0.0002500687741814206 \t | loss: 2.8872\n",
            "Epoch: 7 \t | step: 2776 \t | lr: 9.765551085500107e-05 \t | loss: 3.2088\n",
            "Epoch: 7 \t | step: 3123 \t | lr: 2.0640506168128686e-06 \t | loss: 2.9542\n",
            "Epoch: 7 \t | step: 3470 \t | lr: 0.0002349195030348785 \t | loss: 3.4369\n",
            "--------------------------------------\n",
            "Epoch 7 summary\n",
            "Avg Train loss: 3.3231344772559095\n",
            "Avg Test loss: 3.540328621864319\n",
            "--------------------------------------\n",
            "Epoch: 8 \t | step: 0 \t | lr: 0.0002337529046649841 \t | loss: 2.2766\n",
            "Epoch: 8 \t | step: 347 \t | lr: 7.888975375460822e-05 \t | loss: 3.0196\n",
            "Epoch: 8 \t | step: 694 \t | lr: 0.0002999734416809309 \t | loss: 3.1837\n",
            "Epoch: 8 \t | step: 1041 \t | lr: 0.00021711315189534594 \t | loss: 3.0000\n",
            "Epoch: 8 \t | step: 1388 \t | lr: 6.262610478227527e-05 \t | loss: 3.1568\n",
            "Epoch: 8 \t | step: 1735 \t | lr: 0.00029837326250940825 \t | loss: 3.1225\n",
            "Epoch: 8 \t | step: 2082 \t | lr: 0.0001993697604334139 \t | loss: 3.5134\n",
            "Epoch: 8 \t | step: 2429 \t | lr: 4.781833994161097e-05 \t | loss: 2.5691\n",
            "Epoch: 8 \t | step: 2776 \t | lr: 0.00029432313690211983 \t | loss: 4.2731\n",
            "Epoch: 8 \t | step: 3123 \t | lr: 0.00018081670065579858 \t | loss: 3.0396\n",
            "Epoch: 8 \t | step: 3470 \t | lr: 3.4711792500289304e-05 \t | loss: 3.8361\n",
            "--------------------------------------\n",
            "Epoch 8 summary\n",
            "Avg Train loss: 3.2700413825564794\n",
            "Avg Test loss: 3.3042637515068054\n",
            "--------------------------------------\n",
            "Epoch: 9 \t | step: 0 \t | lr: 3.3825654102919685e-05 \t | loss: 2.4362\n",
            "Epoch: 9 \t | step: 347 \t | lr: 0.00028732855530682716 \t | loss: 2.3720\n",
            "Epoch: 9 \t | step: 694 \t | lr: 0.00016035587689373093 \t | loss: 2.6120\n",
            "Epoch: 9 \t | step: 1041 \t | lr: 2.2785523960955355e-05 \t | loss: 2.8312\n",
            "Epoch: 9 \t | step: 1388 \t | lr: 0.0002784579870168344 \t | loss: 2.2357\n",
            "Epoch: 9 \t | step: 1735 \t | lr: 0.00014111281733036765 \t | loss: 3.8148\n",
            "Epoch: 9 \t | step: 2082 \t | lr: 1.3861351904027867e-05 \t | loss: 3.3569\n",
            "Epoch: 9 \t | step: 2429 \t | lr: 0.0002674674261811999 \t | loss: 2.8839\n",
            "Epoch: 9 \t | step: 2776 \t | lr: 0.0001220252834809371 \t | loss: 2.7436\n",
            "Epoch: 9 \t | step: 3123 \t | lr: 7.200992541902311e-06 \t | loss: 2.5333\n",
            "Epoch: 9 \t | step: 3470 \t | lr: 0.00025453896309055095 \t | loss: 3.3113\n",
            "--------------------------------------\n",
            "Epoch 9 summary\n",
            "Avg Train loss: 3.197570205249671\n",
            "Avg Test loss: 3.3976665425300596\n",
            "--------------------------------------\n",
            "Epoch: 10 \t | step: 0 \t | lr: 0.0002535225143539086 \t | loss: 3.9145\n",
            "Epoch: 10 \t | step: 347 \t | lr: 0.00010207434597937663 \t | loss: 3.0350\n",
            "Epoch: 10 \t | step: 694 \t | lr: 2.6965641616818637e-06 \t | loss: 2.6509\n",
            "Epoch: 10 \t | step: 1041 \t | lr: 0.000238753430428766 \t | loss: 3.4390\n",
            "Epoch: 10 \t | step: 1388 \t | lr: 8.430764445968423e-05 \t | loss: 3.1420\n",
            "Epoch: 10 \t | step: 1735 \t | lr: 1.0361484366708853e-06 \t | loss: 4.1005\n",
            "Epoch: 10 \t | step: 2082 \t | lr: 0.00022252217427820638 \t | loss: 3.3402\n",
            "Epoch: 10 \t | step: 2429 \t | lr: 6.763760992886425e-05 \t | loss: 3.2659\n",
            "Epoch: 10 \t | step: 2776 \t | lr: 0.0002991479681643513 \t | loss: 2.5261\n",
            "Epoch: 10 \t | step: 3123 \t | lr: 0.00020509766340279237 \t | loss: 2.3111\n",
            "Epoch: 10 \t | step: 3470 \t | lr: 5.234042951493302e-05 \t | loss: 2.8664\n",
            "--------------------------------------\n",
            "Epoch 10 summary\n",
            "Avg Train loss: 3.1420460356575868\n",
            "Avg Test loss: 3.20073126912117\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trans_params = sum(p.numel() for p in transformer.parameters())\n",
        "print(f\"\\n Total Parameters = {trans_params}\\n\")\n",
        "\n",
        "# Train the model\n",
        "train_transformer(transformer, 3e-4, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hciM0K0IaM4E",
        "outputId": "41f6f780-c5d3-488d-cb15-cf36625e0c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "English : water\n",
            "Predicted : Das Wasser ist Wasser !\n",
            "German : Wasser\n",
            "BLEU score : 0.0456\n",
            "============================================================\n",
            "English : Cricket\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-01b1ce9143d5>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  token_id = torch.tensor(token_id, device=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted : Wir haben es in dem Kopf ge kam ist, auf dem Kopf ist, ist es in der Gegend es in der Gegend ist, auf dem Kopf ist, ist es in der Gegend es in der Gegend es in dem Kopf bin ich aus dem Kopf bin ich aus dem Kopf\n",
            "German : Kricket\n",
            "BLEU score : 0.0000\n",
            "============================================================\n",
            "English : Sleep\n",
            "Predicted : Die Dinge ist langweilig.\n",
            "German : Schlaf\n",
            "BLEU score : 0.0000\n",
            "============================================================\n",
            "English : I am happy\n",
            "Predicted : Ich bin glücklich.\n",
            "German : Ich bin glücklich\n",
            "BLEU score : 0.2118\n",
            "============================================================\n",
            "English : She walks to school\n",
            "Predicted : Sie geht zur Schule.\n",
            "German : Sie geht zur Schule\n",
            "BLEU score : 0.4315\n",
            "============================================================\n",
            "English : They eat apples\n",
            "Predicted : Sie essen in die Kinder essen.\n",
            "German : Sie essen Äpfel\n",
            "BLEU score : 0.0972\n",
            "============================================================\n",
            "English : He goes to the park\n",
            "Predicted : Er geht ins Park ins Park zur Park zur Arbeit.\n",
            "German : Er geht in den Park\n",
            "BLEU score : 0.0630\n",
            "============================================================\n",
            "English : You play games\n",
            "Predicted : Du spielen ein Spiel st st st st dich doch mit dir spielen im st.\n",
            "German : Du spielst Spiele\n",
            "BLEU score : 0.0166\n",
            "============================================================\n",
            "English : apple\n",
            "Predicted : Apfel ist Apfel ?\n",
            "German : Apfel\n",
            "BLEU score : 0.0610\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "incorrect = [\n",
        "    \"I is happy\",\n",
        "    \"She walk to school\",\n",
        "    \"They eats apple\",\n",
        "    \"He go to park\",\n",
        "    \"You plays\",\n",
        "]\n",
        "\n",
        "English = [\n",
        "    \"water\",\n",
        "    \"Cricket\",\n",
        "    \"Sleep\",\n",
        "    \"I am happy\",\n",
        "    \"She walks to school\",\n",
        "    \"They eat apples\",\n",
        "    \"He goes to the park\",\n",
        "    \"You play games\",\n",
        "    \"apple\"\n",
        "]\n",
        "\n",
        "German = [\n",
        "    \"Wasser\",\n",
        "    \"Kricket\",\n",
        "    \"Schlaf\",\n",
        "    \"Ich bin glücklich\",\n",
        "    \"Sie geht zur Schule\",\n",
        "    \"Sie essen Äpfel\",\n",
        "    \"Er geht in den Park\",\n",
        "    \"Du spielst Spiele\",\n",
        "    \"Apfel\"\n",
        "]\n",
        "\n",
        "for i in range(len(English)):\n",
        "    print(20 * '===')\n",
        "    print(\"English : \" + English[i])\n",
        "\n",
        "    predicted = transformer.predict(English[i])\n",
        "    print(\"Predicted : \" + predicted)\n",
        "    print(\"German : \" + German[i])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = [German[i].split()]\n",
        "    candidate = predicted.split()\n",
        "    bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "    print(f\"BLEU score : {bleu_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZXukxj6yYVZ3"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer, \"transformer_model_full.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2r30r40m2WE",
        "outputId": "f9c333eb-d929-441e-e860-88614269a4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ich liebe dich sehr.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-01b1ce9143d5>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  token_id = torch.tensor(token_id, device=device)\n"
          ]
        }
      ],
      "source": [
        "print(transformer.predict(\"I love you\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
